{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true,
        "id": "L0cOXYIYi8fK"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "VsUUjG8opaeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing dataset"
      ],
      "metadata": {
        "id": "2bR8l_3WL3CM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d moltean/fruits"
      ],
      "metadata": {
        "id": "-H1GgppkpjbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip fruits.zip"
      ],
      "metadata": {
        "id": "0D_vgvOLp8hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "593f6bf3064a52388f6f6dd2df6252980a631e6f",
        "id": "QWIjOnWIi8fM"
      },
      "cell_type": "code",
      "source": [
        "# loading the directories\n",
        "training_dir = '/content/fruits-360_dataset/fruits-360/Training'\n",
        "validation_dir = '/content/fruits-360_dataset/fruits-360/Test'\n",
        "test_dir = '/content/fruits-360_dataset/fruits-360/test-multiple_fruits'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2687d1c9f4a5d3aa631dfac1cd3c26fd0dd3d166",
        "collapsed": true,
        "id": "gq8iao9qi8fM"
      },
      "cell_type": "code",
      "source": [
        "# getting number of files\n",
        "image_files = glob(training_dir + '/*/*.jp*g')\n",
        "valid_image_files = glob(validation_dir + '/*/*.jp*g')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_files = glob(test_dir + '/*.jp*g')"
      ],
      "metadata": {
        "id": "WRBCq9Jfc19L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9ecb46c4d6c9d2cf744b36cfced6398752383a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCQox_3pi8fN",
        "outputId": "e18e8de4-dcff-4a1d-d16a-3d2861be2719"
      },
      "cell_type": "code",
      "source": [
        "# getting the number of classes(types of fruits)\n",
        "folders = glob(training_dir + '/*')\n",
        "num_classes = len(folders)\n",
        "print ('Total Classes = ' + str(num_classes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Classes = 131\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "45d613325b9c0f27d67c44590aa191d67a91050f",
        "collapsed": true,
        "id": "rLM38Qwpi8fO"
      },
      "cell_type": "code",
      "source": [
        "# copying the pre-trained weights to our kernel\n",
        "!mkdir ~/.keras\n",
        "!mkdir ~/.keras/models\n",
        "!cp ../input/keras-pretrained-models/*notop* ~/.keras/models/\n",
        "!cp ../input/keras-pretrained-models/imagenet_class_index.json ~/.keras/models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "tiNUfVX7L6la"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cfdd83abd9a203157499159e0e087c9530aeea1a",
        "id": "e_j-Pysri8fO"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.applications import VGG16\n",
        "\n",
        "IMAGE_SIZE = [64, 64]\n",
        "\n",
        "# loading the weights of VGG16\n",
        "vgg = VGG16(input_shape = IMAGE_SIZE + [3], weights = 'imagenet', include_top = False)\n",
        "\n",
        "for layer in vgg.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = Flatten()(vgg.output)\n",
        "x = Dense(num_classes, activation = 'softmax')(x)\n",
        "\n",
        "model = Model(inputs = vgg.input, outputs = x)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "703eb0b5beabb2508c5381b7f21d048f64712e8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2KPFgI8i8fP",
        "outputId": "8db75ea1-dacf-4643-e06b-cdc55f10e85d"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 64, 64, 64)        1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 64, 64, 64)        36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 32, 32, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 32, 32, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 16, 16, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 16, 16, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 16, 16, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 131)               268419    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14983107 (57.16 MB)\n",
            "Trainable params: 268419 (1.02 MB)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train, test data generation"
      ],
      "metadata": {
        "id": "zRl4gzR6MZpB"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "24bccf942ce0f39558cff2a04a18a6a30ede9998",
        "scrolled": false,
        "id": "Hh5r9IiVi8fQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e941af1f-e174-40ae-d8f0-06bc41829697"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "training_datagen = ImageDataGenerator(\n",
        "                                    rescale=1./255,   # all pixel values will be between 0 an 1\n",
        "                                    shear_range=0.2,\n",
        "                                    zoom_range=0.2,\n",
        "                                    horizontal_flip=True,\n",
        "                                    preprocessing_function=preprocess_input)\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n",
        "\n",
        "training_generator = training_datagen.flow_from_directory(training_dir, target_size = IMAGE_SIZE, batch_size = 200, class_mode = 'categorical')\n",
        "validation_generator = validation_datagen.flow_from_directory(validation_dir, target_size = IMAGE_SIZE, batch_size = 200, class_mode = 'categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 67692 images belonging to 131 classes.\n",
            "Found 22688 images belonging to 131 classes.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "96302bd261bd293fe19b02ae88f2ab5ddaa75d07",
        "id": "EF0SqWPNi8fR"
      },
      "cell_type": "code",
      "source": [
        "# checking the labels\n",
        "training_generator.class_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "GFk1O4yBNeb-"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "ffb1b7ce1f7560753f83a10de47cf290fbec9159",
        "_kg_hide-output": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPnIoiwNi8fS",
        "outputId": "382ae48b-0dc5-4bf6-c586-c1664a2a12e6"
      },
      "cell_type": "code",
      "source": [
        "training_images = 37836\n",
        "validation_images = 12709\n",
        "\n",
        "history = model.fit_generator(training_generator,\n",
        "                   epochs = 1,\n",
        "                   validation_data = validation_generator\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-b930add88be4>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(training_generator,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "339/339 [==============================] - 3943s 12s/step - loss: 1.5133 - accuracy: 0.7549 - val_loss: 0.9250 - val_accuracy: 0.8293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction"
      ],
      "metadata": {
        "id": "8jAPK3MhMgP3"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c147c541d4c6d71d03cf2011c6933486b05db1da",
        "id": "HaajaE3di8fS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d010e596-e37a-4690-ebec-a08786bd171f"
      },
      "cell_type": "code",
      "source": [
        "print ('Training Accuracy = ' + str(history.history['accuracy']))\n",
        "print ('Validation Accuracy = ' + str(history.history['val_accuracy']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy = [0.7548750042915344]\n",
            "Validation Accuracy = [0.8292930126190186]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model\n",
        "import pickle\n",
        "\n",
        "with open('vgg16.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "oHKeFyRKuG-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the model\n",
        "import pickle\n",
        "\n",
        "pkl_file_path = '/content/vgg16.pkl'\n",
        "with open(pkl_file_path, 'rb') as file:\n",
        "    model = pickle.load(file)\n"
      ],
      "metadata": {
        "id": "2BSkwT5aNHuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_weights = []\n",
        "for layer in model.layers:\n",
        "    if hasattr(layer, 'get_weights'):\n",
        "        weights = layer.get_weights()\n",
        "        all_weights.append(weights)\n",
        "\n",
        "for i, weights in enumerate(all_weights):\n",
        "    print(f\"Layer {i + 1} - {model.layers[i].name}\")\n",
        "    for j, w in enumerate(weights):\n",
        "        print(f\"   Weights {j + 1}: {w.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDLi8zGqNTcx",
        "outputId": "340492b5-1de5-42c8-82a6-9dd62a5f4346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1 - input_1\n",
            "Layer 2 - block1_conv1\n",
            "   Weights 1: (3, 3, 3, 64)\n",
            "   Weights 2: (64,)\n",
            "Layer 3 - block1_conv2\n",
            "   Weights 1: (3, 3, 64, 64)\n",
            "   Weights 2: (64,)\n",
            "Layer 4 - block1_pool\n",
            "Layer 5 - block2_conv1\n",
            "   Weights 1: (3, 3, 64, 128)\n",
            "   Weights 2: (128,)\n",
            "Layer 6 - block2_conv2\n",
            "   Weights 1: (3, 3, 128, 128)\n",
            "   Weights 2: (128,)\n",
            "Layer 7 - block2_pool\n",
            "Layer 8 - block3_conv1\n",
            "   Weights 1: (3, 3, 128, 256)\n",
            "   Weights 2: (256,)\n",
            "Layer 9 - block3_conv2\n",
            "   Weights 1: (3, 3, 256, 256)\n",
            "   Weights 2: (256,)\n",
            "Layer 10 - block3_conv3\n",
            "   Weights 1: (3, 3, 256, 256)\n",
            "   Weights 2: (256,)\n",
            "Layer 11 - block3_pool\n",
            "Layer 12 - block4_conv1\n",
            "   Weights 1: (3, 3, 256, 512)\n",
            "   Weights 2: (512,)\n",
            "Layer 13 - block4_conv2\n",
            "   Weights 1: (3, 3, 512, 512)\n",
            "   Weights 2: (512,)\n",
            "Layer 14 - block4_conv3\n",
            "   Weights 1: (3, 3, 512, 512)\n",
            "   Weights 2: (512,)\n",
            "Layer 15 - block4_pool\n",
            "Layer 16 - block5_conv1\n",
            "   Weights 1: (3, 3, 512, 512)\n",
            "   Weights 2: (512,)\n",
            "Layer 17 - block5_conv2\n",
            "   Weights 1: (3, 3, 512, 512)\n",
            "   Weights 2: (512,)\n",
            "Layer 18 - block5_conv3\n",
            "   Weights 1: (3, 3, 512, 512)\n",
            "   Weights 2: (512,)\n",
            "Layer 19 - block5_pool\n",
            "Layer 20 - flatten\n",
            "Layer 21 - dense\n",
            "   Weights 1: (2048, 131)\n",
            "   Weights 2: (131,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "extracting weights, biases of each layer"
      ],
      "metadata": {
        "id": "4vpxEM-1NjsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv1_weights, conv1_biases = model.layers[1].get_weights()\n",
        "conv2_weights, conv2_biases = model.layers[2].get_weights()\n",
        "conv3_weights, conv3_biases = model.layers[4].get_weights()\n",
        "conv4_weights, conv4_biases = model.layers[5].get_weights()\n",
        "conv5_weights, conv5_biases = model.layers[7].get_weights()\n",
        "conv6_weights, conv6_biases = model.layers[8].get_weights()\n",
        "conv7_weights, conv7_biases = model.layers[9].get_weights()\n",
        "conv8_weights, conv8_biases = model.layers[11].get_weights()\n",
        "conv9_weights, conv9_biases = model.layers[12].get_weights()\n",
        "conv10_weights, conv10_biases = model.layers[13].get_weights()\n",
        "conv11_weights, conv11_biases = model.layers[15].get_weights()\n",
        "conv12_weights, conv12_biases = model.layers[16].get_weights()\n",
        "conv13_weights, conv13_biases = model.layers[17].get_weights()\n",
        "dense1_weights, dense1_biases = model.layers[20].get_weights()"
      ],
      "metadata": {
        "id": "91PBVPmhNr6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense1_weights_flattened_list = dense1_weights.flatten().tolist()"
      ],
      "metadata": {
        "id": "x0SiltuKRvwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting biases to lists\n",
        "conv1_biases_list = conv1_biases.tolist()\n",
        "conv2_biases_list = conv2_biases.tolist()\n",
        "conv3_biases_list = conv3_biases.tolist()\n",
        "conv4_biases_list = conv4_biases.tolist()\n",
        "conv5_biases_list = conv5_biases.tolist()\n",
        "conv6_biases_list = conv6_biases.tolist()\n",
        "conv7_biases_list = conv7_biases.tolist()\n",
        "conv8_biases_list = conv8_biases.tolist()\n",
        "conv9_biases_list = conv9_biases.tolist()\n",
        "conv10_biases_list = conv10_biases.tolist()\n",
        "conv11_biases_list = conv11_biases.tolist()\n",
        "conv12_biases_list = conv12_biases.tolist()\n",
        "conv13_biases_list = conv13_biases.tolist()\n",
        "dense1_biases_list = dense1_biases.tolist()"
      ],
      "metadata": {
        "id": "MHnjyn7ROW-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantization of weights"
      ],
      "metadata": {
        "id": "5M42IS9WNsh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def absmax_quantize_list(weights_list):\n",
        "    X = torch.tensor(weights_list, dtype=torch.float32)\n",
        "    scale = 127 / torch.max(torch.abs(X))\n",
        "\n",
        "    # Quantize\n",
        "    X_quant = (scale * X).round()\n",
        "\n",
        "    # Dequantize\n",
        "    X_dequant = X_quant / scale\n",
        "\n",
        "    quantized_list = X_quant.to(torch.int8).tolist()\n",
        "    return quantized_list, X_dequant.tolist()"
      ],
      "metadata": {
        "id": "xplRLGvGRNFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_conv_weights = []\n",
        "for num_fil in range(0,512):\n",
        "  for dep in range(0,512):\n",
        "    for row in range(0,3):\n",
        "      for col in range(0,3):\n",
        "        new_conv_weights.append(conv13_weights[row][col][dep][num_fil])"
      ],
      "metadata": {
        "id": "tlP9iUvSTFA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_list = new_conv_weights\n",
        "print(len(weights_list))\n",
        "\n",
        "quantized_weights, dequantized_weights = absmax_quantize_list(weights_list)\n",
        "\n",
        "print(\"\\nAbsmax quantized weights:\")\n",
        "print(len(quantized_weights))\n",
        "\n",
        "with open('quantized_conv13_weights.txt', 'w') as f:\n",
        "  f.write(str(quantized_weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26N6uTfqRS_V",
        "outputId": "69a0e570-1203-47f4-b164-667efdf12f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2359296\n",
            "\n",
            "Absmax quantized weights:\n",
            "2359296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "xd_DHvIcJk8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "image_path = '/content/fruits-360_dataset/fruits-360/Training/Dates/13_100.jpg'\n",
        "new_width = 64\n",
        "new_height = 64\n",
        "\n",
        "image = Image.open(image_path)\n",
        "resized_image = image.resize((new_width, new_height))\n",
        "random_image = np.array(resized_image)\n",
        "\n",
        "print(random_image)"
      ],
      "metadata": {
        "id": "qxr6HklPSNEQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3db006b7-5d05-48f5-b4c9-9f79ca3cf4c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flattenedImage = []\n",
        "for k in range(0, 3):\n",
        "  for i in range(0, 64):\n",
        "    for j in range(0, 64):\n",
        "      flattenedImage.append(random_image[i][j][k])\n"
      ],
      "metadata": {
        "id": "RH3btmhZJnY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(random_image.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGFim9PNKqVo",
        "outputId": "72e5ac2e-dd0a-43f5-b3aa-904120b326c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 64, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('test_image.txt', 'w') as f:\n",
        "  f.write(str(flattenedImage))"
      ],
      "metadata": {
        "id": "B5e7mqIqKsJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flattening the image\n",
        "def flatten(image, h, w, d):\n",
        "  flattenedImage = []\n",
        "  for k in range(0, d):\n",
        "    for i in range(0, h):\n",
        "      for j in range(0, w):\n",
        "        flattenedImage.append(image[i][j][k])\n",
        "  return flattenedImage"
      ],
      "metadata": {
        "id": "Oz7xUHXTWQmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "taking some samples"
      ],
      "metadata": {
        "id": "y1tllYwmWcus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "test_images = []\n",
        "true_labels = []\n",
        "tdata = []\n",
        "\n",
        "for idx in range(0, 100):\n",
        "    image = Image.open(test_files[idx])\n",
        "    resized_image = image.resize((64, 64))\n",
        "    reshaped_image = np.array(resized_image)\n",
        "    # tdata.append(x_test[idx])\n",
        "    test_images.append(flatten(reshaped_image, 64, 64, 3))\n",
        "    # true_labels.append(y_test[idx])\n",
        "\n",
        "flattened_test_images = [item for sublist in test_images for item in sublist]"
      ],
      "metadata": {
        "id": "eiwCImJRWTsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(flattened_test_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X05aVstMgncW",
        "outputId": "26a0488b-77fd-4128-db62-bc7f8adf55e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1228800"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('test_images.txt', 'w') as f:\n",
        "  f.write(str(flattened_test_images))"
      ],
      "metadata": {
        "id": "4hz0je2IWWrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from tensorflow.keras.preprocessing import image\n",
        "image_filenames = test_files[0:100]\n",
        "all_images = []\n",
        "for filename in image_filenames:\n",
        "    img = image.load_img(filename, target_size=(64, 64))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = img_array / 255.0\n",
        "    all_images.append(img_array)\n"
      ],
      "metadata": {
        "id": "Mejc5XGZsATU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time calculation"
      ],
      "metadata": {
        "id": "siT9icPYWqDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_images = np.array(all_images)\n",
        "predictions = model.predict(all_images)\n",
        "\n",
        "start_time = time.time()\n",
        "predictions = model.predict(all_images)\n",
        "end_time = time.time()\n",
        "predicted_class_index = []\n",
        "for i in range(len(predictions)):\n",
        "  predicted_class_index.append(np.argmax(predictions[i]))\n",
        "inference_time = []\n",
        "inference_time.append(end_time - start_time)\n",
        "print(f\"Inference time: {inference_time[0]} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUVmqBJXvYuw",
        "outputId": "3da68a06-4e60-473e-ec0f-75592e5801cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 4s 969ms/step\n",
            "4/4 [==============================] - 5s 1s/step\n",
            "Inference time: 5.170581817626953 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('predicted_labels.txt', 'w') as f:\n",
        "  f.write(str(predicted_class_index))"
      ],
      "metadata": {
        "id": "CqS7Hfj5v71K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}